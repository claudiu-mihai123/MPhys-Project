{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot as up\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import vector\n",
    "#import zfit\n",
    "import mplhep as hep\n",
    "import yaml\n",
    "import seaborn as sns\n",
    "import particle\n",
    "import vector\n",
    "#from CMS_cuts import CMS_cut_func\n",
    "from uncertainties import ufloat\n",
    "hep.style.use('ATLAS')\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import BinaryCrossentropy\n",
    "import pandas as pd\n",
    "intum=3\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import uncertainties.unumpy as unp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_coeff(a,b):\n",
    "    a_resample=[]\n",
    "    b_resample=[]\n",
    "    for i in range(len(unp.nominal_values(a))):\n",
    "        if unp.nominal_values(a)[i]<0:\n",
    "\n",
    "            a_bootstrap=np.random.poisson(np.abs(unp.nominal_values(a)[i]),1000)\n",
    "            a_resample.append(-a_bootstrap)\n",
    "        else:\n",
    "            a_bootstrap=np.random.poisson(unp.nominal_values(a)[i],1000)\n",
    "            a_resample.append(a_bootstrap)\n",
    "        if unp.nominal_values(b)[i]<0:\n",
    "\n",
    "            b_bootstrap=np.random.poisson(np.abs(unp.nominal_values(b)[i]),1000)\n",
    "            b_resample.append(-b_bootstrap)\n",
    "        else:\n",
    "            b_bootstrap=np.random.poisson(unp.nominal_values(b)[i],1000)\n",
    "            b_resample.append(b_bootstrap)\n",
    "        \n",
    "\n",
    "    a_resample=np.concatenate(a_resample)\n",
    "    b_resample=np.concatenate(b_resample)\n",
    "    cov = np.cov(a_resample, b_resample, ddof=0)[0, 1]\n",
    "    sigma_a = np.std(a_resample)\n",
    "    sigma_b = np.std(b_resample)\n",
    "    rho = cov / (sigma_a * sigma_b)\n",
    "    return rho\n",
    "def error_propagation(a,operation,b):\n",
    "    rho=corr_coeff(a,b)\n",
    "    a_nom=unp.nominal_values(a)\n",
    "    b_nom=unp.nominal_values(b)\n",
    "    a_err=unp.std_devs(a)\n",
    "    b_err=unp.std_devs(b)\n",
    "\n",
    "\n",
    "    if operation==\"/\":\n",
    "        \n",
    "        result=a_nom/b_nom\n",
    "\n",
    "        result_err=np.sqrt(  (a_err/b_nom)**2 +((a_nom*b_err/b_nom**2)**2  ))\n",
    "\n",
    "        \n",
    "\n",
    "    if operation=='-':\n",
    "\n",
    "        result=a_nom-b_nom\n",
    "\n",
    "        result_err=np.sqrt(a_err**2+b_err**2 - 2*rho*a_err*b_err)\n",
    "\n",
    "\n",
    "\n",
    "    if operation=='+':\n",
    "        result=a_nom+b_nom\n",
    "\n",
    "        result_err=np.sqrt(a_err**2+ b_err**2 + 2*1*a_err*b_err)\n",
    "    \n",
    "    return unp.uarray(result,result_err)\n",
    "\n",
    "\n",
    "def error_propagation_test(a,operation,b,rho):\n",
    "    a_nom=unp.nominal_values(a)\n",
    "    b_nom=unp.nominal_values(b)\n",
    "    a_err=unp.std_devs(a)\n",
    "    b_err=unp.std_devs(b)\n",
    "\n",
    "\n",
    "    if operation==\"/\":\n",
    "        \n",
    "        result=a_nom/b_nom\n",
    "\n",
    "        result_err=np.sqrt(  (a_err/b_nom)**2 +((a_nom*b_err/b_nom**2)**2  ))\n",
    "\n",
    "        \n",
    "\n",
    "    if operation=='-':\n",
    "\n",
    "        result=a_nom-b_nom\n",
    "\n",
    "        result_err=np.sqrt(a_err**2+b_err**2 - 2*rho*a_err*b_err)\n",
    "        print(rho)\n",
    "\n",
    "\n",
    "    if operation=='+':\n",
    "        result=a_nom+b_nom\n",
    "\n",
    "        result_err=np.sqrt(a_err**2+ b_err**2 + 2*rho*a_err*b_err)\n",
    "    \n",
    "    return unp.uarray(result,result_err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(labels, num_classes=None):\n",
    "\n",
    "    # get num_classes from max label if None\n",
    "    if num_classes is None:\n",
    "        num_classes = int(np.max(labels)) + 1\n",
    "\n",
    "    y = np.asarray(labels, dtype=int)\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes))\n",
    "\n",
    "    # index into array and set appropriate values to 1\n",
    "    categorical[np.arange(n), y] = 1\n",
    "\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_det, sim_part, data_det, data_part = pd.read_csv('/Users/claudiumihai/MPhys/Analysis/Semester2/DataFiles/SM/DataSM_Detector_level_new_jet_veto.csv'),\\\n",
    "    pd.read_csv('/Users/claudiumihai/MPhys/Analysis/Semester2/DataFiles/SM/DataSM_Particle_level_new_jet_veto.csv'),\\\n",
    "        pd.read_csv('/Users/claudiumihai/MPhys/Analysis/Semester2/DataFiles/cwtil10/DataEFT_cp_odd_cwtil1_Detector_level.csv'),\\\n",
    "              pd.read_csv('/Users/claudiumihai/MPhys/Analysis/Semester2/DataFiles/cwtil10/DataEFT_cp_odd_cwtil1_Particle_level.csv')\n",
    "lumi=138\n",
    "tree=up.open('/Users/claudiumihai/MPhys/Analysis/Semester2/DataFiles/ROOT Files/SM/CMS_500k.root:Delphes')\n",
    "w_full_sm=tree.arrays(['Event.Weight'],library='ak')\n",
    "sim_part['Event.Weight']=w_full_sm[sim_part['Unnamed: 0']]['Event.Weight']\n",
    "sim_const=lumi*0.9962*1000/np.sum(w_full_sm['Event.Weight'])\n",
    "tree=up.open('/Users/claudiumihai/MPhys/Analysis/Semester2/DataFiles/ROOT Files/EFT/CP_odd/EFT_cp_odd_cwtil1.root:Delphes')\n",
    "w_full_np1=tree.arrays(['Event.Weight'],library='ak')\n",
    "data_part['Event.Weight']=w_full_np1[data_part['Unnamed: 0']]['Event.Weight']\n",
    "\n",
    "data_const=lumi*1.153*1000/np.sum(w_full_np1['Event.Weight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_det['Lepton.PT']=sim_det.apply(lambda row : row ['Electron.PT'] if row['Electron_size']==1 else row['Muon.PT'],axis=1)\n",
    "sim_det['Lepton.Eta']=sim_det.apply(lambda row : row ['Electron.Eta'] if row['Electron_size']==1 else row['Muon.Eta'],axis=1)\n",
    "sim_det['Lepton.Phi']=sim_det.apply(lambda row : row ['Electron.Phi'] if row['Electron_size']==1 else row['Muon.Phi'],axis=1)\n",
    "sim_part['Lepton.PT']=sim_part.apply(lambda row : row ['Electron.PT'] if row['Electron_size']==1 else row['Muon.PT'],axis=1)\n",
    "sim_part['Lepton.Eta']=sim_part.apply(lambda row : row ['Electron.Eta'] if row['Electron_size']==1 else row['Muon.Eta'],axis=1)\n",
    "sim_part['Lepton.Phi']=sim_part.apply(lambda row : row ['Electron.Phi'] if row['Electron_size']==1 else row['Muon.Phi'],axis=1)\n",
    "data_det['Lepton.PT']=data_det.apply(lambda row : row ['Electron.PT'] if row['Electron_size']==1 else row['Muon.PT'],axis=1)\n",
    "data_det['Lepton.Eta']=data_det.apply(lambda row : row ['Electron.Eta'] if row['Electron_size']==1 else row['Muon.Eta'],axis=1)\n",
    "data_det['Lepton.Phi']=data_det.apply(lambda row : row ['Electron.Phi'] if row['Electron_size']==1 else row['Muon.Phi'],axis=1)\n",
    "data_part['Lepton.PT']=data_part.apply(lambda row : row ['Electron.PT'] if row['Electron_size']==1 else row['Muon.PT'],axis=1)\n",
    "data_part['Lepton.Eta']=data_part.apply(lambda row : row ['Electron.Eta'] if row['Electron_size']==1 else row['Muon.Eta'],axis=1)\n",
    "data_part['Lepton.Phi']=data_part.apply(lambda row : row ['Electron.Phi'] if row['Electron_size']==1 else row['Muon.Phi'],axis=1)\n",
    "data_det=data_det.loc[(data_det['Lepton.PT']<300) &(data_det['Photon.PT']<300)  & (data_det['MissingET.MET']<300)]\n",
    "data_part=data_part.loc[(data_part['Lepton.PT']<300) &(data_part['Photon.PT']<300)  & (data_part['MissingET.MET']<300)]\n",
    "sim_det=sim_det.loc[(sim_det['Lepton.PT']<300) &(sim_det['Photon.PT']<300)  & (sim_det['MissingET.MET']<300)]\n",
    "sim_part=sim_part.loc[(sim_part['Lepton.PT']<300) &(sim_part['Photon.PT']<300)  & (sim_part['MissingET.MET']<300)]\n",
    "\n",
    "\n",
    "unfold_vars=['Lepton.PT','Lepton.Phi','Lepton.Eta','Photon.PT','Photon.Eta','Photon.Phi','MissingET.MET','MissingET.Phi']\n",
    "\n",
    "\n",
    "X_det=np.asarray([np.concatenate((data_det[var],sim_det[var])) for var in unfold_vars]).T\n",
    "Y_det=to_categorical(np.concatenate(( np.ones(len(data_det['Lepton.Eta'])),\n",
    "                                             np.zeros(len(sim_det['Lepton.Eta']))   )))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_gen=np.asarray([np.concatenate((sim_part[var],sim_part[var])) for var in unfold_vars]).T\n",
    "Y_gen=to_categorical(np.concatenate(( np.ones(len(sim_part['Lepton.Eta'])),\n",
    "                                             np.zeros(len(sim_part['Lepton.Eta']))   )))\n",
    "\n",
    "\n",
    "ndata, nsim = np.count_nonzero(Y_det[:,1]), np.count_nonzero(Y_det[:,0])\n",
    "wdata = np.ones(ndata)\n",
    "winit = ndata/nsim*np.ones(nsim)\n",
    "sim_det['event']=sim_det['Unnamed: 0']\n",
    "sim_part['event']=sim_part['Unnamed: 0']\n",
    "data_det['event']=None\n",
    "input_det=pd.concat([pd.DataFrame(data_det[unfold_vars+['event']]),pd.DataFrame(sim_det[unfold_vars+['event']])])\n",
    "input_det['target']=np.concatenate((np.ones(len(data_det)),np.zeros(len(sim_det))))\n",
    "input_det['w']=np.concatenate((np.ones(len(data_det)),winit*  sim_det['Event.Weight'])  )\n",
    "\n",
    "input_gen=pd.concat([pd.DataFrame(sim_part[unfold_vars+['event']]),pd.DataFrame(sim_part[unfold_vars+['event']])])\n",
    "input_gen['target']=np.concatenate((np.ones(len(sim_part)),np.zeros(len(sim_part))))\n",
    "input_gen['w']=np.concatenate((np.ones(len(sim_part)),np.ones(len(sim_part))))\n",
    "D=np.array(sim_det['Unnamed: 0'])\n",
    "P=np.array(sim_part['Unnamed: 0'])                              \n",
    "full_set_events=np.unique(np.concatenate((D,P)))\n",
    "common_events=np.intersect1d(D,P)\n",
    "det_eff_events=np.setdiff1d(P,D)\n",
    "neutrino_eff_events=np.setdiff1d(D,P)\n",
    "particle_events=P\n",
    "\n",
    "def normalize_columns(df, columns):\n",
    "    df_normalized = df[columns].copy()  # Work only on the specified columns\n",
    "    for col in columns:\n",
    "        min_val = df[col].min()\n",
    "        max_val = df[col].max()\n",
    "        if max_val - min_val == 0:  # Avoid division by zero\n",
    "            df_normalized[col] = 0\n",
    "        else:\n",
    "            df_normalized[col] = (df[col] - min_val) / (max_val - min_val)\n",
    "    return df_normalized\n",
    "\n",
    "def normalize_columns_to_minus1_1(df, columns):\n",
    "    df_normalized = df[columns].copy()  # Work only on the specified columns\n",
    "    for col in columns:\n",
    "        min_val = df[col].min()\n",
    "        max_val = df[col].max()\n",
    "        if max_val - min_val == 0:  # Avoid division by zero\n",
    "            df_normalized[col] = 0\n",
    "        else:\n",
    "            # Normalize to [-1, 1]\n",
    "            df_normalized[col] = 2 * (df[col] - min_val) / (max_val - min_val) - 1\n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45791345, 0.412319  , 0.59548107, ..., 0.82213769, 0.57819755,\n",
       "       0.750144  ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open('/Users/claudiumihai/MPhys/Analysis/Semester2/DataFiles/100NN_weights/coin_flip.pkl','rb') as file:\n",
    "    coin_flip=pickle.load(file)\n",
    "coin_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_det['coin_flip']=coin_flip[:len(data_det)]\n",
    "sim_det['coin_flip']=coin_flip[:len(sim_det)]\n",
    "sim_part['coin_flip']=coin_flip[:len(sim_part)]\n",
    "data_part['coin_flip']=coin_flip[:len(data_part)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CP-sensitive Observables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_detector(data_df):\n",
    "    W_boson=particle.Particle.from_pdgid(24)\n",
    "    M_w=W_boson.mass/1000\n",
    "\n",
    "    data_df['large_boost']=0.0\n",
    "    data_df.loc[data_df['Electron_size']==1,'large_boost']=M_w**2/(data_df['Electron.PT']*data_df['MissingET.MET'])\n",
    "    data_df.loc[data_df['Muon_size']==1,'large_boost']=M_w**2/(data_df['Muon.PT']*data_df['MissingET.MET'])\n",
    "\n",
    "    data_df['delta']=0.0\n",
    "    data_df.loc[data_df['Electron_size']==1,'delta']=(M_w**2-data_df['Mt_lv']**2)/(2*data_df['Electron.PT']*data_df['MissingET.MET'])\n",
    "    data_df.loc[data_df['Muon_size']==1,'delta']=(M_w**2-data_df['Mt_lv']**2)/(2*data_df['Muon.PT']*data_df['MissingET.MET'])\n",
    "\n",
    "\n",
    "    data_df['lepton_mass']=0.0\n",
    "    data_df.loc[data_df['Muon_size']==1,'lepton_mass']=particle.Particle.from_pdgid(13).mass/1000\n",
    "    data_df.loc[data_df['Electron_size']==1,'lepton_mass']=particle.Particle.from_pdgid(11).mass/1000\n",
    "    data_df['Mt_lv']=0.0\n",
    "\n",
    "\n",
    "    data_df.loc[data_df['Electron_size']==1,'Mt_lv']=np.sqrt(  \n",
    "        ( data_df['MissingET.MET']+ np.sqrt((data_df['Electron.PT'])**2+data_df['lepton_mass']**2))**2 \n",
    "        - (   data_df['Electron.PT']*np.cos(data_df['Electron.Phi']) +data_df['MissingET.MET']*np.cos(data_df['MissingET.Phi'])  )**2\n",
    "            - (   data_df['Electron.PT']*np.sin(data_df['Electron.Phi']) +data_df['MissingET.MET']*np.sin(data_df['MissingET.Phi'])  )**2   )\n",
    "\n",
    "    data_df.loc[data_df['Muon_size']==1,'Mt_lv']=np.sqrt(  \n",
    "        ( data_df['MissingET.MET']+ np.sqrt((data_df['Muon.PT'])**2+data_df['lepton_mass']**2))**2 \n",
    "        - (   data_df['Muon.PT']*np.cos(data_df['Muon.Phi']) +data_df['MissingET.MET']*np.cos(data_df['MissingET.Phi'])  )**2\n",
    "            - (   data_df['Muon.PT']*np.sin(data_df['Muon.Phi']) +data_df['MissingET.MET']*np.sin(data_df['MissingET.Phi'])  )**2   )\n",
    "\n",
    "\n",
    "    data_df['large_boost']=0.0\n",
    "    data_df.loc[data_df['Electron_size']==1,'large_boost']=M_w**2/(data_df['Electron.PT']*data_df['MissingET.MET'])\n",
    "    data_df.loc[data_df['Muon_size']==1,'large_boost']=M_w**2/(data_df['Muon.PT']*data_df['MissingET.MET'])\n",
    "\n",
    "    data_df['delta']=0.0\n",
    "    data_df.loc[data_df['Electron_size']==1,'delta']=(M_w**2-data_df['Mt_lv']**2)/(2*data_df['Electron.PT']*data_df['MissingET.MET'])\n",
    "    data_df.loc[data_df['Muon_size']==1,'delta']=(M_w**2-data_df['Mt_lv']**2)/(2*data_df['Muon.PT']*data_df['MissingET.MET'])\n",
    "\n",
    "\n",
    "    data_df['sign']=np.sign(data_df['delta'])\n",
    "    data_df.loc[(data_df['sign']==-1.0) & (data_df['Electron_size']==1),'Eta_v']=data_df['Electron.Eta']\n",
    "    data_df.loc[(data_df['sign']==-1.0) & (data_df['Muon_size']==1),'Eta_v']=data_df['Muon.Eta']\n",
    "\n",
    "    def eta_based_on_coin_flip_electron(row):\n",
    "        eta_plus = row['Electron.Eta'] + np.log(1 + np.sqrt(row['delta']) * np.sqrt(2 + row['delta']) + row['delta'])\n",
    "        eta_minus = row['Electron.Eta'] - np.log(1 + np.sqrt(row['delta']) * np.sqrt(2 + row['delta']) + row['delta'])\n",
    "        # Select based on coin_flip value\n",
    "        return eta_plus if row['coin_flip']  <0.5 else eta_minus #np.random.choice([eta_plus,eta_minus])#e\n",
    "\n",
    "    data_df.loc[(data_df['Electron_size'] == 1) & (data_df['delta'] >= 0), 'Eta_v'] = data_df[\n",
    "        (data_df['Electron_size'] == 1) & (data_df['delta'] >= 0)\n",
    "    ].apply(eta_based_on_coin_flip_electron, axis=1)\n",
    "\n",
    "    def eta_based_on_coin_flip_muon(row):\n",
    "        eta_plus = row['Muon.Eta'] + np.log(1 + np.sqrt(row['delta']) * np.sqrt(2 + row['delta']) + row['delta'])\n",
    "        eta_minus = row['Muon.Eta'] - np.log(1 + np.sqrt(row['delta']) * np.sqrt(2 + row['delta']) + row['delta'])\n",
    "        # Select based on coin_flip value\n",
    "        return eta_plus if row['coin_flip'] <0.5 else eta_minus #np.random.choice([eta_plus,eta_minus]) #e\n",
    "\n",
    "    # Apply the updated function\n",
    "    data_df.loc[(data_df['Muon_size'] == 1) & (data_df['delta'] >= 0), 'Eta_v'] = data_df[\n",
    "        (data_df['Muon_size'] == 1) & (data_df['delta'] >= 0)\n",
    "    ].apply(eta_based_on_coin_flip_muon, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def phi_transf(phi):\n",
    "        return np.piecewise(\n",
    "        phi,\n",
    "        [\n",
    "            phi < -np.pi / 2,  \n",
    "            np.abs(phi) < np.pi / 2,  \n",
    "            phi > np.pi / 2  \n",
    "        ],\n",
    "        [\n",
    "            lambda phi: -(np.pi + phi),  \n",
    "            lambda phi: phi,  \n",
    "            lambda phi: np.pi - phi  \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def phi_recon(charge,particle):\n",
    "\n",
    "        p_l=vector.zip({'pt':data_df[particle+'.PT'].loc[(data_df[particle+'_size']==1) & (data_df[particle+'.Charge']==charge)] ,'eta':data_df[particle+'.Eta'].loc[(data_df[particle+'_size']==1) & (data_df[particle+'.Charge']==charge)],\n",
    "                        'phi':data_df[particle+'.Phi'].loc[(data_df[particle+'_size']==1) & (data_df[particle+'.Charge']==charge)],'mass':data_df['lepton_mass'].loc[(data_df[particle+'_size']==1) & (data_df[particle+'.Charge']==charge)]  })\n",
    "        p_y=vector.zip({'pt':data_df['Photon.PT'].loc[(data_df[particle+'_size']==1) & (data_df[particle+'.Charge']==charge)] ,'eta':data_df['Photon.Eta'].loc[(data_df[particle+'_size']==1) & (data_df[particle+'.Charge']==charge)],\n",
    "                        'phi':data_df['Photon.Phi'].loc[(data_df[particle+'_size']==1) & (data_df[particle+'.Charge']==charge)],'mass':0 })\n",
    "\n",
    "        p_vl=vector.zip({'pt':data_df['MissingET.MET'].loc[(data_df[particle+'_size']==1) & (data_df[particle+'.Charge']==charge)] ,'eta':data_df['Eta_v'].loc[(data_df[particle+'_size']==1) & (data_df[particle+'.Charge']==charge)],\n",
    "                    'phi':data_df['MissingET.Phi'].loc[(data_df[particle+'_size']==1) & (data_df[particle+'.Charge']==charge)],'mass':0 })\n",
    "\n",
    "\n",
    "        p_w=p_l+p_vl\n",
    "        p_com=p_w+p_y\n",
    "        p_w_boosted=p_w.boost(-p_com.to_beta3())\n",
    "        p_l_boosted=p_l.boost(-p_com.to_beta3())\n",
    "        p_vl_boosted=p_vl.boost(-p_com.to_beta3())\n",
    "\n",
    "        n1=p_w_boosted.to_Vector3D().cross(-p_com.to_Vector3D())    \n",
    "        n1=n1/n1.mag\n",
    "\n",
    "        n_ref=n1.cross(p_w_boosted.to_Vector3D())\n",
    "        n_ref=n_ref/n_ref.mag\n",
    "        if charge==1.0:\n",
    "            n2=-p_l_boosted.to_Vector3D().cross(p_w_boosted.to_Vector3D())\n",
    "        elif charge==-1.0:\n",
    "            n2=-p_vl_boosted.to_Vector3D().cross(p_w_boosted.to_Vector3D())\n",
    "        else: return 'error'\n",
    "\n",
    "        n2=n2/n2.mag   \n",
    "\n",
    "        x=n2.dot(n1)\n",
    "        y=n2.dot(n_ref)\n",
    "\n",
    "        phi=-np.arctan2(y,x)\n",
    "        phi=np.array(phi)\n",
    "\n",
    "        return phi\n",
    "\n",
    "\n",
    "\n",
    "    data_df['Phi']=0.0\n",
    "\n",
    "    data_df.loc[(data_df['Electron_size']==1) & (data_df['Electron.Charge']==1.0),'phi']=phi_recon(1.0,'Electron')\n",
    "    data_df.loc[(data_df['Electron_size']==1) & (data_df['Electron.Charge']==-1.0),'phi']=phi_recon(-1.0,'Electron')\n",
    "\n",
    "    data_df.loc[(data_df['Muon_size']==1) & (data_df['Muon.Charge']==1.0),'phi']=phi_recon(1.0,'Muon')\n",
    "    data_df.loc[(data_df['Muon_size']==1) & (data_df['Muon.Charge']==-1.0),'phi']=phi_recon(-1.0,'Muon')\n",
    "\n",
    "    return data_df\n",
    "\n",
    "def phi_particle(data_df):\n",
    "\n",
    "\n",
    "    def phi(phi):\n",
    "        return np.piecewise(\n",
    "        phi,\n",
    "        [\n",
    "            phi < -np.pi / 2,  \n",
    "            np.abs(phi) < np.pi / 2,  \n",
    "            phi > np.pi / 2  \n",
    "        ],\n",
    "        [\n",
    "            lambda phi: -(np.pi + phi),  \n",
    "            lambda phi: phi,  \n",
    "            lambda phi: np.pi - phi  \n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def phi_recon(charge,particle):\n",
    "\n",
    "        p_l=vector.zip({'pt':data_df[particle+'.PT'].loc[(data_df[particle+'_size']==1) & (data_df[particle+'.Charge']==charge)] ,'eta':data_df[particle+'.Eta'].loc[(data_df[particle+'_size']==1) & (data_df[particle+'.Charge']==charge)],\n",
    "                        'phi':data_df[particle+'.Phi'].loc[(data_df[particle+'_size']==1) & (data_df[particle+'.Charge']==charge)],'mass':data_df['lepton_mass'].loc[(data_df[particle+'_size']==1) & (data_df[particle+'.Charge']==charge)]  })\n",
    "        p_y=vector.zip({'pt':data_df['Photon.PT'].loc[(data_df[particle+'_size']==1) & (data_df[particle+'.Charge']==charge)] ,'eta':data_df['Photon.Eta'].loc[(data_df[particle+'_size']==1) & (data_df[particle+'.Charge']==charge)],\n",
    "                        'phi':data_df['Photon.Phi'].loc[(data_df[particle+'_size']==1) & (data_df[particle+'.Charge']==charge)],'mass':0 })\n",
    "\n",
    "        p_vl=vector.zip({'pt':data_df['MissingET.MET'].loc[(data_df[particle+'_size']==1) & (data_df[particle+'.Charge']==charge)] ,'eta':data_df['MissingET.Eta'].loc[(data_df[particle+'_size']==1) & (data_df[particle+'.Charge']==charge)],\n",
    "                    'phi':data_df['MissingET.Phi'].loc[(data_df[particle+'_size']==1) & (data_df[particle+'.Charge']==charge)],'mass':0 })\n",
    "\n",
    "\n",
    "        p_w=p_l+p_vl\n",
    "        p_com=p_w+p_y\n",
    "        p_w_boosted=p_w.boost(-p_com.to_beta3())\n",
    "        p_l_boosted=p_l.boost(-p_com.to_beta3())\n",
    "        p_vl_boosted=p_vl.boost(-p_com.to_beta3())\n",
    "\n",
    "        n1=p_w_boosted.to_Vector3D().cross(-p_com.to_Vector3D())    \n",
    "        n1=n1/n1.mag\n",
    "\n",
    "        n_ref=n1.cross(p_w_boosted.to_Vector3D())\n",
    "        n_ref=n_ref/n_ref.mag\n",
    "        if charge==1.0:\n",
    "            n2=-p_l_boosted.to_Vector3D().cross(p_w_boosted.to_Vector3D())\n",
    "        elif charge==-1.0:\n",
    "            n2=-p_vl_boosted.to_Vector3D().cross(p_w_boosted.to_Vector3D())\n",
    "        else: return 'error'\n",
    "\n",
    "        n2=n2/n2.mag   \n",
    "\n",
    "        x=n2.dot(n1)\n",
    "        y=n2.dot(n_ref)\n",
    "\n",
    "        phi=-np.arctan2(y,x)\n",
    "        phi=np.array(phi)\n",
    "\n",
    "        return phi\n",
    "\n",
    "\n",
    "\n",
    "    data_df['Phi']=0.0\n",
    "\n",
    "    data_df.loc[(data_df['Electron_size']==1) & (data_df['Electron.Charge']==1.0),'phi_true']=phi_recon(1.0,'Electron')\n",
    "    data_df.loc[(data_df['Electron_size']==1) & (data_df['Electron.Charge']==-1.0),'phi_true']=phi_recon(-1.0,'Electron')\n",
    "\n",
    "    data_df.loc[(data_df['Muon_size']==1) & (data_df['Muon.Charge']==1.0),'phi_true']=phi_recon(1.0,'Muon')\n",
    "    data_df.loc[(data_df['Muon_size']==1) & (data_df['Muon.Charge']==-1.0),'phi_true']=phi_recon(-1.0,'Muon')\n",
    "\n",
    "    return data_df\n",
    "\n",
    "def dPhi_l_y(data_df):\n",
    "    for i in data_df.index:\n",
    "        if data_df['Lepton.Eta'][i]>data_df['Photon.Eta'][i]:\n",
    "            temp=data_df['Lepton.Phi'][i]-data_df['Photon.Phi'][i]\n",
    "            if temp>np.pi:\n",
    "                data_df.at[i,'dPhi']=temp-np.pi\n",
    "            elif temp<-np.pi:\n",
    "                data_df.at[i,'dPhi']=temp+np.pi\n",
    "            else:\n",
    "                data_df.at[i,'dPhi']=temp\n",
    "        else:\n",
    "            temp=data_df['Photon.Phi'][i]-data_df['Lepton.Phi'][i]\n",
    "            if temp>np.pi:\n",
    "                data_df.at[i,'dPhi']=temp-np.pi\n",
    "            elif temp<-np.pi:\n",
    "                 data_df.at[i,'dPhi']=temp+np.pi\n",
    "            else:\n",
    "                 data_df.at[i,'dPhi']=temp\n",
    "            \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_part=phi_detector(sim_part)\n",
    "sim_det=phi_detector(sim_det)\n",
    "\n",
    "data_part=phi_detector(data_part)\n",
    "data_det=phi_detector(data_det)\n",
    "\n",
    "\n",
    "sim_part=phi_particle(sim_part)\n",
    "data_part=phi_particle(data_part)\n",
    "\n",
    "\n",
    "\n",
    "sim_det=dPhi_l_y(sim_det)\n",
    "sim_part=dPhi_l_y(sim_part)\n",
    "data_det=dPhi_l_y(data_det)\n",
    "data_part=dPhi_l_y(data_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Omnifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406us/step\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372us/step\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 821us/step\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 398us/step\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423us/step\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416us/step\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424us/step\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 443us/step\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370us/step\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 798us/step\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 402us/step\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448us/step\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 391us/step\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 881us/step\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 407us/step\n",
      "\u001b[1m109/109\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 416us/step\n",
      "\u001b[1m117/117\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373us/step\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 797us/step\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 405us/step\n"
     ]
    }
   ],
   "source": [
    "unfold_vars=['Lepton.Phi','Lepton.Eta','Photon.Phi','Photon.Eta','MissingET.Phi','Lepton.PT','MissingET.MET','Photon.PT','Eta_v']\n",
    "unfold_vars_norm=[i+'_std' for i in unfold_vars]\n",
    "\n",
    "weights_push=np.ones(len(sim_part['Lepton.PT']))\n",
    "weights_pull=np.ones(len(sim_part['Lepton.PT']))\n",
    "\n",
    "\n",
    "data_det[unfold_vars_norm]  = normalize_columns(data_det,unfold_vars)\n",
    "sim_det[unfold_vars_norm]  = normalize_columns(sim_det,unfold_vars)\n",
    "data_part[unfold_vars_norm]  = normalize_columns(data_part,unfold_vars)\n",
    "sim_part[unfold_vars_norm]  = normalize_columns(sim_part,unfold_vars)\n",
    "\n",
    "\n",
    "input_det=pd.concat([pd.DataFrame(data_det[unfold_vars_norm+['event']]),pd.DataFrame(sim_det[unfold_vars_norm+['event']])])\n",
    "input_det['target']=np.concatenate((np.ones(len(data_det)),np.zeros(len(sim_det))))\n",
    "\n",
    "input_gen=pd.concat([pd.DataFrame(sim_part[unfold_vars_norm+['event']]),pd.DataFrame(sim_part[unfold_vars_norm+['event']])])\n",
    "input_gen['target']=np.concatenate((np.ones(len(sim_part)),np.zeros(len(sim_part))))\n",
    "\n",
    "\n",
    "n=len(unfold_vars)\n",
    "\n",
    "\n",
    "inputs_step1 = Input((n, ))\n",
    "hidden_layer_1_step1 = Dense(100, activation='relu')(inputs_step1)\n",
    "hidden_layer_2_step1 = Dense(100, activation='relu')(hidden_layer_1_step1)\n",
    "hidden_layer_3_step1 = Dense(100, activation='relu')(hidden_layer_2_step1)\n",
    "\n",
    "outputs_step1 = Dense(1, activation='sigmoid')(hidden_layer_1_step1)\n",
    "model_step1 = Model(inputs=inputs_step1, outputs=outputs_step1)\n",
    "\n",
    "\n",
    "weights_push=np.ones(len(full_set_events))\n",
    "weights_pull=np.ones(len(full_set_events))\n",
    "\n",
    "push_mapping=dict(zip(full_set_events,weights_push))\n",
    "pull_mapping=dict(zip(full_set_events,weights_pull))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs_step2 = Input((n, ))\n",
    "hidden_layer_1_step2 = Dense(100, activation='relu')(inputs_step2)\n",
    "hidden_layer_2_step2 = Dense(100, activation='relu')(hidden_layer_1_step2)\n",
    "hidden_layer_3_step2 = Dense(100, activation='relu')(hidden_layer_2_step2)\n",
    "\n",
    "outputs_step2 = Dense(1, activation='sigmoid')(hidden_layer_3_step2)\n",
    "model_step2 = Model(inputs=inputs_step2, outputs=outputs_step2)\n",
    "\n",
    "push_series=pd.Series(push_mapping)\n",
    "pull_series=pd.Series(pull_mapping)\n",
    "push_series.loc[D]=np.array(sim_det['Event.Weight'])*winit\n",
    "data_weights=np.array(data_det['Event.Weight'])\n",
    "for i in range(5):\n",
    "\n",
    "    # Step 1\n",
    "\n",
    "    w1=np.concatenate((data_weights,push_series.loc[D]))\n",
    "\n",
    "    model_step1 = Model(inputs=inputs_step1, outputs=outputs_step1)\n",
    "    X_train,X_test,y_train,y_test,w_train,w_test=train_test_split(input_det[unfold_vars_norm],\n",
    "                                                                input_det['target'],\n",
    "                                                                w1)\n",
    "\n",
    "    model_step1.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "\n",
    "    model_step1.fit(X_train,y_train,sample_weight=w_train,epochs=50,batch_size=1500,validation_data=(X_test,y_test,w_test),verbose=False)\n",
    "\n",
    "    f=model_step1.predict(sim_det.loc[sim_det['event'].isin(common_events)][unfold_vars_norm],batch_size=700)\n",
    "    \n",
    "    temp=f/(1.-f)\n",
    "    temp=np.squeeze(np.nan_to_num(temp)) \n",
    "\n",
    "    pull_series.loc[common_events]=temp*push_series.loc[common_events]\n",
    "\n",
    "\n",
    "    # estimation 1\n",
    "\n",
    "    estimation_1_X=pd.concat([sim_part.loc[sim_part['event'].isin(common_events)][unfold_vars_norm],sim_part.loc[sim_part['event'].isin(common_events)][unfold_vars_norm]])\n",
    "\n",
    "    estimation_1_Y=np.concatenate((np.ones(len(sim_part.loc[sim_part['event'].isin(common_events)][unfold_vars_norm])), np.zeros(len(sim_part.loc[sim_part['event'].isin(common_events)][unfold_vars_norm]))))\n",
    "    \n",
    "    estimation_1_w=np.concatenate((pull_series.loc[common_events],np.ones(len(sim_part.loc[sim_part['event'].isin(common_events)][unfold_vars_norm]))))\n",
    "\n",
    "    X_t, X_v, y_t, y_v, w_t, w_v = train_test_split(estimation_1_X,estimation_1_Y,estimation_1_w)\n",
    "\n",
    "    model_estimation_1=Model(inputs=inputs_step1, outputs=outputs_step1)\n",
    "    model_estimation_1.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "\n",
    "    model_estimation_1.fit(X_t,y_t,sample_weight=w_t,epochs=50,batch_size=700,validation_data=(X_v,y_v,w_v),verbose=False)\n",
    "\n",
    "    f=model_estimation_1.predict(sim_part.loc[sim_part['event'].isin(det_eff_events)][unfold_vars_norm],batch_size=400)\n",
    "\n",
    "    temp=f/(1.-f)\n",
    "\n",
    "    temp=np.squeeze(np.nan_to_num(temp)) \n",
    "\n",
    "    pull_series.loc[det_eff_events]=temp\n",
    "\n",
    "    # Step 2\n",
    "    w2=np.concatenate((pull_series[P],push_series[P]))\n",
    "\n",
    "    model_step2 = Model(inputs=inputs_step2, outputs=outputs_step2)\n",
    "\n",
    "    X_train,X_test,y_train,y_test,w_train,w_test=train_test_split(input_gen[unfold_vars_norm],\n",
    "                                                                input_gen['target'],\n",
    "                                                                w2)\n",
    "    model_step2.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "\n",
    "    model_step2.fit(X_train,y_train,sample_weight=w_train,epochs=50,batch_size=2000,validation_data=(X_test,y_test,w_test),verbose=False)\n",
    "\n",
    "\n",
    "    \n",
    "    f=model_step2.predict(sim_part[unfold_vars_norm],batch_size=1000)\n",
    "    \n",
    "    temp=f/(1.-f)\n",
    "\n",
    "    temp=np.squeeze(np.nan_to_num(temp)) \n",
    "    \n",
    "    \n",
    "    push_series.loc[P]=temp*push_series[P]\n",
    "\n",
    "\n",
    "    # Estimation 2\n",
    "\n",
    "    estimation_2_X=pd.concat([sim_det.loc[sim_det['event'].isin(common_events)][unfold_vars_norm],sim_det.loc[sim_det['event'].isin(common_events)][unfold_vars_norm]])\n",
    "\n",
    "    estimation_2_Y=np.concatenate((np.ones(len(sim_det.loc[sim_det['event'].isin(common_events)][unfold_vars_norm])), np.zeros(len(sim_det.loc[sim_det['event'].isin(common_events)][unfold_vars_norm]))))\n",
    "    \n",
    "    estimation_2_w=np.concatenate((push_series.loc[common_events],np.ones(len(sim_det.loc[sim_det['event'].isin(common_events)][unfold_vars_norm]))))\n",
    "\n",
    "    X_t, X_v, y_t, y_v, w_t, w_v = train_test_split(estimation_2_X,estimation_2_Y,estimation_2_w)\n",
    "\n",
    "    model_estimation_2=Model(inputs=inputs_step2, outputs=outputs_step2)\n",
    "    model_estimation_2.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])\n",
    "\n",
    "    model_estimation_2.fit(X_t,y_t,sample_weight=w_t,epochs=50,batch_size=700,validation_data=(X_v,y_v,w_v),verbose=False)\n",
    "\n",
    "    f=model_estimation_2.predict(sim_det.loc[sim_det['event'].isin(neutrino_eff_events)][unfold_vars_norm],batch_size=50)\n",
    "\n",
    "    temp=f/(1.-f)\n",
    "\n",
    "    temp=np.squeeze(np.nan_to_num(temp)) \n",
    "\n",
    "    push_series.loc[neutrino_eff_events]=temp\n",
    "\n",
    "    # Output\n",
    "w_push=np.array(push_series.loc[P])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_to_test=[w_push]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfold_vars=['Lepton.Phi','Lepton.Eta','Photon.Phi','Photon.Eta','MissingET.Phi','Lepton.PT','MissingET.MET','Photon.PT','Eta_v']\n",
    "all_vars=unfold_vars+['phi','dPhi']\n",
    "scores={key: {} for key in all_vars}\n",
    "\n",
    "\n",
    "for w in weights_to_test:\n",
    "    sim_part['omni']=w\n",
    "    data_sets={'SM':{'particle':sim_part,'detector':sim_det,'weights_detector':sim_det['Event.Weight'],'weights_particle':sim_part['Event.Weight'],'constant':sim_const,'weights_full':w_full_sm},\n",
    "           'EFT_NP1':{'particle':data_part,'detector':data_det,'weights_detector':data_det['Event.Weight'],'weights_particle':data_part['Event.Weight'],'constant':data_const,'weights_full':w_full_np1},}\n",
    "    sim_part_copy=sim_part.loc[\n",
    "    (sim_part['Lepton.PT'] > 0) & (sim_part['Lepton.PT'] <300) &\n",
    "    (sim_part['Photon.PT'] > 0) & (sim_part['Photon.PT'] <300) &\n",
    "    (sim_part['MissingET.MET'] > 0) & (sim_part['MissingET.MET'] <300)\n",
    "    ].copy()\n",
    "\n",
    "\n",
    "    sim_det_copy=sim_det.loc[\n",
    "        (sim_det['Lepton.PT'] > 0) & (sim_det['Lepton.PT'] <300) &\n",
    "        (sim_det['Photon.PT'] > 0) & (sim_det['Photon.PT'] <300) &\n",
    "        (sim_det['MissingET.MET'] > 0) & (sim_det['MissingET.MET'] <300)\n",
    "    ].copy()\n",
    "\n",
    "\n",
    "    data_part_copy=data_part.loc[\n",
    "        (data_part['Lepton.PT'] > 0) & (data_part['Lepton.PT'] <300) &\n",
    "        (data_part['Photon.PT'] > 0) & (data_part['Photon.PT'] <300) &\n",
    "        (data_part['MissingET.MET'] > 0) & (data_part['MissingET.MET'] <300)\n",
    "    ].copy()\n",
    "\n",
    "\n",
    "\n",
    "    data_det_copy=data_det.loc[\n",
    "        (data_det['Lepton.PT'] > 0) & (data_det['Lepton.PT'] <300) &\n",
    "        (data_det['Photon.PT'] > 0) & (data_det['Photon.PT'] <300) &\n",
    "        (data_det['MissingET.MET'] > 0) & (data_det['MissingET.MET'] <300)\n",
    "    ].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    data_sets={'SM':{'particle':sim_part_copy,'detector':sim_det_copy,'weights_detector':sim_det_copy['Event.Weight'],'weights_particle':sim_part_copy['Event.Weight'],'constant':sim_const,'weights_full':w_full_sm},\n",
    "            'EFT_NP1':{'particle':data_part_copy,'detector':data_det_copy,'weights_detector':data_det_copy['Event.Weight'],'weights_particle':data_part_copy['Event.Weight'],'constant':data_const,'weights_full':w_full_np1},}\n",
    "\n",
    "\n",
    "    from hist_plot import hist_plot\n",
    "\n",
    "    scores_temp={}\n",
    "    for var in all_vars:\n",
    "        \n",
    "\n",
    "        sim_det_temp=data_sets['SM']['detector'][var]\n",
    "        sim_part_temp=data_sets['SM']['particle'][var]\n",
    "        data_det_temp=data_sets['EFT_NP1']['detector'][var]\n",
    "        data_part_temp=data_sets['EFT_NP1']['particle'][var]\n",
    "\n",
    "        omni_weights=data_sets['SM']['particle']['omni']*sim_const\n",
    "        result_data_det_temp=hist_plot(data=data_det_temp,weights=data_sets['EFT_NP1']['weights_detector'],bins=20,constant=data_sets['EFT_NP1']['constant'],weights_full=data_sets['EFT_NP1']['weights_full'])\n",
    "\n",
    "        \n",
    "        result_sim_part_temp=hist_plot(data=sim_part_temp,weights=data_sets['SM']['weights_particle'],bins=20,constant=data_sets['SM']['constant'],weights_full=data_sets['SM']['weights_full'])\n",
    "\n",
    "\n",
    "        result_data_part_temp=hist_plot(data=data_part_temp,weights=data_sets['EFT_NP1']['weights_particle'],bins=20,constant=data_sets['EFT_NP1']['constant'],weights_full=data_sets['EFT_NP1']['weights_full'])\n",
    "\n",
    "        result_sim_part_temp_omni=hist_plot(data=sim_part_temp,weights=data_sets['SM']['particle']['omni'],bins=20,constant=data_sets['SM']['constant'],weights_full=data_sets['SM']['weights_full'])\n",
    "\n",
    "        metric_omni=error_propagation(result_data_part_temp['expected_norm'],'/',result_sim_part_temp_omni['expected_norm'])\n",
    "\n",
    "        metric=error_propagation(result_data_part_temp['expected_norm'],'/',result_sim_part_temp['expected_norm'])\n",
    "    \n",
    "        #if (var=='dPhi') or (var=='phi'):\n",
    "        chi=np.sum( (np.square(unp.nominal_values(metric)-1))/unp.std_devs(metric) )\n",
    "        chi_omni=np.sum( (np.square(unp.nominal_values(metric_omni)-1))/unp.std_devs(metric_omni) )\n",
    "        scores_temp[var]=[chi,chi_omni]\n",
    "\n",
    "        omni_iter='omni'+'_'+str(weights_to_test.index(w))\n",
    "\n",
    "        scores[var]['base']=chi\n",
    "        scores[var][omni_iter]=chi_omni\n",
    "\n",
    "    #if (scores['dPhi'][-1]<base_score['dPhi']*0.2)  and  (scores['phi'][-1]<base_score['phi']*0.2)    :\n",
    "    #   good_weights.append(w)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lepton.Phi': {'base': 0.4325475007528058, 'omni_0': 0.49982478966705185},\n",
       " 'Lepton.Eta': {'base': 0.20996827122821105, 'omni_0': 0.5936189607938415},\n",
       " 'Photon.Phi': {'base': 0.578695465583572, 'omni_0': 0.5419955563317573},\n",
       " 'Photon.Eta': {'base': 0.3879829210827911, 'omni_0': 0.9229497435543266},\n",
       " 'MissingET.Phi': {'base': 0.2893657934529643, 'omni_0': 0.42096583468738213},\n",
       " 'Lepton.PT': {'base': 124.86666674779167, 'omni_0': 3.421142665865231},\n",
       " 'MissingET.MET': {'base': 159.7886771425295, 'omni_0': 3.535423210421304},\n",
       " 'Photon.PT': {'base': 285.4410635028072, 'omni_0': 1.038791798401853},\n",
       " 'Eta_v': {'base': 6.650360050344657, 'omni_0': 3.2322755506240988},\n",
       " 'phi': {'base': 4.174137002693565, 'omni_0': 0.7130374885774639},\n",
       " 'dPhi': {'base': 28.02647185567154, 'omni_0': 0.6321273659917592}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lepton.Phi</th>\n",
       "      <th>Lepton.Eta</th>\n",
       "      <th>Photon.Phi</th>\n",
       "      <th>Photon.Eta</th>\n",
       "      <th>MissingET.Phi</th>\n",
       "      <th>Lepton.PT</th>\n",
       "      <th>MissingET.MET</th>\n",
       "      <th>Photon.PT</th>\n",
       "      <th>Eta_v</th>\n",
       "      <th>phi</th>\n",
       "      <th>dPhi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>base</th>\n",
       "      <td>0.432548</td>\n",
       "      <td>0.209968</td>\n",
       "      <td>0.578695</td>\n",
       "      <td>0.387983</td>\n",
       "      <td>0.289366</td>\n",
       "      <td>124.866667</td>\n",
       "      <td>159.788677</td>\n",
       "      <td>285.441064</td>\n",
       "      <td>6.650360</td>\n",
       "      <td>4.174137</td>\n",
       "      <td>28.026472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>omni_0</th>\n",
       "      <td>0.499825</td>\n",
       "      <td>0.593619</td>\n",
       "      <td>0.541996</td>\n",
       "      <td>0.922950</td>\n",
       "      <td>0.420966</td>\n",
       "      <td>3.421143</td>\n",
       "      <td>3.535423</td>\n",
       "      <td>1.038792</td>\n",
       "      <td>3.232276</td>\n",
       "      <td>0.713037</td>\n",
       "      <td>0.632127</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Lepton.Phi  Lepton.Eta  Photon.Phi  Photon.Eta  MissingET.Phi  \\\n",
       "base      0.432548    0.209968    0.578695    0.387983       0.289366   \n",
       "omni_0    0.499825    0.593619    0.541996    0.922950       0.420966   \n",
       "\n",
       "         Lepton.PT  MissingET.MET   Photon.PT     Eta_v       phi       dPhi  \n",
       "base    124.866667     159.788677  285.441064  6.650360  4.174137  28.026472  \n",
       "omni_0    3.421143       3.535423    1.038792  3.232276  0.713037   0.632127  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df=pd.DataFrame(scores)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
